title: Google Colab Execution Guide for PMU Instance Sampah Fuzzy
type: text/markdown
tags: yolo training inference onnx rknn troubleshooting deployment configuration documentation guide reference git cuda python structure docs documentation
created: 20250807170027
modified: 20250807220000
source: docs/COLAB_EXECUTION_GUIDE.md

# Google Colab Execution Guide for PMU Instance Sampah Fuzzy

This guide provides step-by-step instructions for executing the waste detection system on Google Colab, updated to align with `requirements-dev-linux-rknn-local.txt` and use the specific RKNN Toolkit2 wheel from GitHub.

## üìã Prerequisites

Before starting, ensure you have:
- A Google account
- Access to Google Colab
- A Roboflow API key (for dataset access)
- Basic understanding of Python and machine learning concepts

## üöÄ Step-by-Step Execution Guide

### Step 1: Open Google Colab

1. Go to [Google Colab](https://colab.research.google.com/)
2. Sign in with your Google account
3. Create a new notebook or open an existing one

### Step 2: Clone the Repository

Run this cell to clone your repository:

```python
# Clone the repository
!git clone https://github.com/trisgelar/pmu-instance-sampah-fuzzy.git

# Navigate to the project directory
%cd pmu-instance-sampah-fuzzy

# List contents to verify
!ls -la
```

### Step 3: Install Dependencies (Updated)

Install all required packages aligned with `requirements-dev-linux-rknn-local.txt`:

```python
# Core dependencies from requirements file
core_deps = [
    "ultralytics==8.3.174",
    "roboflow==1.2.3", 
    "opencv-python==4.11.0.86",
    "matplotlib==3.10.5",
    "seaborn==0.13.2",
    "pandas==2.3.1",
    "numpy==1.26.4",
    "PyYAML==6.0.2",
    "scikit-fuzzy==0.5.0",
    "tensorboard==2.20.0",
    "onnx==1.18.0",
    "onnxruntime==1.22.1",
    "torch==2.4.0+cu124",
    "torchvision==0.19.0+cu124",
    "torchaudio==2.4.0+cu124",
    "pillow==11.3.0",
    "requests==2.32.4",
    "python-dotenv==1.1.1",
    "tqdm==4.67.1",
    "protobuf==4.25.4",
    "flatbuffers==25.2.10",
    "filelock==3.13.1",
    "psutil==7.0.0",
    "scipy==1.16.1",
    "networkx==3.3",
    "kiwisolver==1.4.8",
    "cycler==0.12.1",
    "contourpy==1.3.3",
    "fonttools==4.59.0",
    "markdown==3.8.2",
    "jinja2==3.1.4",
    "packaging==25.0",
    "python-dateutil==2.9.0.post0",
    "pytz==2025.2",
    "six==1.17.0",
    "urllib3==2.5.0",
    "certifi==2025.8.3",
    "charset-normalizer==3.4.2",
    "idna==3.7",
    "typing_extensions==4.12.2",
    "sympy==1.13.3",
    "mpmath==1.3.0",
    "fsspec==2024.6.1",
    "gitdb==4.0.12",
    "GitPython==3.1.45",
    "smmap==5.0.2",
    "grpcio==1.74.0",
    "absl-py==2.3.1",
    "antlr4-python3-runtime==4.9.3",
    "asttokens==3.0.0",
    "coloredlogs==15.0.1",
    "decorator==5.2.1",
    "executing==2.2.0",
    "fast-histogram==0.14",
    "filetype==1.2.0",
    "humanfriendly==10.0",
    "hydra-core==1.3.2",
    "ipython==9.4.0",
    "ipython_pygments_lexers==1.1.1",
    "jedi==0.19.2",
    "markupsafe==2.1.5",
    "matplotlib-inline==0.1.7",
    "omegaconf==2.3.0",
    "parso==0.8.4",
    "pexpect==4.9.0",
    "pi_heif==1.1.0",
    "pillow-avif-plugin==1.5.2",
    "prompt_toolkit==3.0.51",
    "ptyprocess==0.7.0",
    "pure_eval==0.2.3",
    "py-cpuinfo==9.0.0",
    "pygments==2.19.2",
    "pyparsing==3.2.3",
    "requests-toolbelt==1.0.0",
    "ruamel.yaml==0.18.14",
    "ruamel.yaml.clib==0.2.12",
    "stack-data==0.6.3",
    "tensorboard-data-server==0.7.2",
    "thop==0.1.1.post2209072238",
    "triton==3.0.0",
    "tzdata==2025.2",
    "ultralytics-thop==2.0.15",
    "wcwidth==0.2.13",
    "werkzeug==3.1.3"
]

# Install core dependencies
print("üì¶ Installing core dependencies...")
for dep in core_deps:
    !pip install {dep}

# Install CUDA dependencies for GPU support
cuda_deps = [
    "nvidia-cublas-cu12==12.4.2.65",
    "nvidia-cuda-cupti-cu12==12.4.99",
    "nvidia-cuda-nvrtc-cu12==12.4.99",
    "nvidia-cuda-runtime-cu12==12.4.99",
    "nvidia-cudnn-cu12==9.1.0.70",
    "nvidia-cufft-cu12==11.2.0.44",
    "nvidia-curand-cu12==10.3.5.119",
    "nvidia-cusolver-cu12==11.6.0.99",
    "nvidia-cusparse-cu12==12.3.0.142",
    "nvidia-nccl-cu12==2.20.5",
    "nvidia-nvjitlink-cu12==12.4.99",
    "nvidia-nvtx-cu12==12.4.99"
]

print("üöÄ Installing CUDA dependencies...")
for dep in cuda_deps:
    !pip install {dep}

# Install RKNN Toolkit2 from GitHub wheel
print("üì¶ Installing RKNN Toolkit2 from GitHub...")
rknn_wheel_url = "https://github.com/airockchip/rknn-toolkit2/raw/master/rknn-toolkit2/packages/x86_64/rknn_toolkit2-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"
!pip install {rknn_wheel_url}

print("‚úÖ All dependencies installed successfully!")
```

### Step 4: Mount Google Drive (Optional but Recommended)

Mount your Google Drive to save results:

```python
from google.colab import drive
drive.mount('/content/drive')

# Create a directory for results
!mkdir -p /content/drive/MyDrive/pmu-waste-detection-results
```

### Step 5: Create Secrets Configuration

Create the `secrets.yaml` file with your API keys:

```python
import yaml

# Create secrets.yaml file
secrets_data = {
    'roboflow_api_key': 'YOUR_ROBOFLOW_API_KEY_HERE'  # Replace with your actual API key
}

with open('secrets.yaml', 'w') as f:
    yaml.dump(secrets_data, f)

print("secrets.yaml created successfully!")
```

**Important**: Replace `'YOUR_ROBOFLOW_API_KEY_HERE'` with your actual Roboflow API key.

### Step 6: Verify Configuration

Check if the configuration files are properly set up:

```python
# Verify configuration files exist
import os

print("Checking configuration files...")
print(f"config.yaml exists: {os.path.exists('config.yaml')}")
print(f"secrets.yaml exists: {os.path.exists('secrets.yaml')}")

# Display configuration structure
if os.path.exists('config.yaml'):
    with open('config.yaml', 'r') as f:
        print("\nConfiguration file contents:")
        print(f.read())

# Verify key dependencies
try:
    import ultralytics
    import roboflow
    import torch
    import onnx
    print("‚úÖ Key dependencies verified")
except ImportError as e:
    print(f"‚ùå Missing dependency: {e}")
```

### Step 7: Import and Initialize the System

Import the main system and initialize it:

```python
# Import the main system
from main_colab import WasteDetectionSystemColab

# Initialize the system
try:
    system = WasteDetectionSystemColab()
    print("‚úÖ System initialized successfully!")
    
    # Display system status
    status = system.get_system_status()
    print(f"System Status: {status}")
    
except Exception as e:
    print(f"‚ùå System initialization failed: {e}")
```

### Step 8: Check CUDA/GPU Status

Verify GPU availability and optimize settings:

```python
# Check CUDA status
cuda_status = system.get_cuda_status()
print("CUDA Status:")
print(f"Memory Info: {cuda_status.get('memory_info', 'N/A')}")
print(f"Optimal Batch Size: {cuda_status.get('optimal_batch_size', 'N/A')}")

# Clear CUDA cache if needed
if 'error' not in cuda_status:
    system.clear_cuda_cache()
    print("‚úÖ CUDA cache cleared")
```

### Step 9: Prepare Datasets

Download and prepare your datasets:

```python
# Prepare datasets
try:
    system.dataset_manager.prepare_datasets()
    system.dataset_manager.zip_datasets_folder()
    print("‚úÖ Datasets prepared successfully!")
except Exception as e:
    print(f"‚ùå Dataset preparation failed: {e}")
```

### Step 10: Train YOLOv8n Model

Train the first model (YOLOv8n):

```python
# Train YOLOv8n model
try:
    print("üöÄ Starting YOLOv8n training...")
    run_dir_v8n = system.train_and_export_model("v8n", epochs=50, batch_size=16)
    
    if run_dir_v8n:
        print(f"‚úÖ YOLOv8n training completed! Run directory: {run_dir_v8n}")
    else:
        print("‚ùå YOLOv8n training failed")
        
except Exception as e:
    print(f"‚ùå YOLOv8n training failed: {e}")
```

### Step 11: Analyze Training Results

Analyze the training metrics:

```python
# Analyze training results
if run_dir_v8n:
    try:
        system.analyze_training_run(run_dir_v8n, "v8n")
        print("‚úÖ Training analysis completed!")
    except Exception as e:
        print(f"‚ùå Training analysis failed: {e}")
```

### Step 12: Run Inference and Visualization

Run inference on sample images:

```python
# Run inference and visualization
if run_dir_v8n:
    try:
        system.run_inference_and_visualization(run_dir_v8n, "v8n", num_inference_images=6)
        print("‚úÖ Inference and visualization completed!")
    except Exception as e:
        print(f"‚ùå Inference failed: {e}")
```

### Step 13: Train Additional Models (Optional)

Train YOLOv10n and YOLOv11n models:

```python
# Train YOLOv10n model
try:
    print("üöÄ Starting YOLOv10n training...")
    run_dir_v10n = system.train_and_export_model("v10n", epochs=50, batch_size=16)
    
    if run_dir_v10n:
        print(f"‚úÖ YOLOv10n training completed!")
        system.analyze_training_run(run_dir_v10n, "v10n")
        system.run_inference_and_visualization(run_dir_v10n, "v10n", num_inference_images=6)
    else:
        print("‚ùå YOLOv10n training failed")
        
except Exception as e:
    print(f"‚ùå YOLOv10n training failed: {e}")

# Train YOLOv11n model
try:
    print("üöÄ Starting YOLOv11n training...")
    run_dir_v11n = system.train_and_export_model("v11n", epochs=50, batch_size=16)
    
    if run_dir_v11n:
        print(f"‚úÖ YOLOv11n training completed!")
        system.analyze_training_run(run_dir_v11n, "v11n")
        system.run_inference_and_visualization(run_dir_v11n, "v11n", num_inference_images=6)
    else:
        print("‚ùå YOLOv11n training failed")
        
except Exception as e:
    print(f"‚ùå YOLOv11n training failed: {e}")
```

### Step 14: Convert to RKNN Models (Updated)

Convert models to RKNN format for edge deployment using the GitHub wheel:

```python
# Convert to RKNN models
try:
    system.convert_and_zip_rknn_models("v8n")
    print("‚úÖ RKNN conversion completed for YOLOv8n!")
    print("üì¶ RKNN Toolkit2 installed from GitHub wheel")
    print("üîó Source: https://github.com/airockchip/rknn-toolkit2")
except Exception as e:
    print(f"‚ùå RKNN conversion failed: {e}")
    print("‚ö†Ô∏è  RKNN conversion is optional. ONNX models are still available.")
```

### Step 15: Save Results to Google Drive

Save all results to your Google Drive:

```python
# Save results to Google Drive
try:
    system.drive_manager.save_all_results_to_drive(folder_to_save="pmu-waste-detection-results")
    print("‚úÖ Results saved to Google Drive!")
except Exception as e:
    print(f"‚ùå Failed to save to Drive: {e}")
```

### Step 16: Monitor Training with TensorBoard

Monitor training progress in real-time:

```python
# Load TensorBoard extension
%load_ext tensorboard

# Start TensorBoard
%tensorboard --logdir runs
```

## üìä Expected Outputs

After successful execution, you should have:

### Generated Files:
- **Model Weights**: `.pt` files in `runs/train/exp*/weights/`
- **ONNX Models**: `.onnx` files in `onnx_models/`
- **RKNN Models**: `.rknn` files in `rknn_models/` (if conversion enabled)
- **Compressed Archives**: `.zip` files for easy download
- **Inference Results**: Images and CSV files with detection results
- **Training Metrics**: Plots and tables showing model performance

### Directory Structure:
```
pmu-instance-sampah-fuzzy/
‚îú‚îÄ‚îÄ runs/
‚îÇ   ‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ exp*/  # Training runs
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ detect/  # Detection results
‚îú‚îÄ‚îÄ onnx_models/
‚îÇ   ‚îú‚îÄ‚îÄ yolov8n_is.onnx
‚îÇ   ‚îú‚îÄ‚îÄ yolov10n_is.onnx
‚îÇ   ‚îî‚îÄ‚îÄ yolov11n_is.onnx
‚îú‚îÄ‚îÄ rknn_models/
‚îÇ   ‚îú‚îÄ‚îÄ yolov8n_is.rknn
‚îÇ   ‚îú‚îÄ‚îÄ yolov10n_is.rknn
‚îÇ   ‚îî‚îÄ‚îÄ yolov11n_is.rknn
‚îú‚îÄ‚îÄ inference_results/
‚îÇ   ‚îú‚îÄ‚îÄ superimposed_images/
‚îÇ   ‚îî‚îÄ‚îÄ inference_results.csv
‚îî‚îÄ‚îÄ datasets/
    ‚îî‚îÄ‚îÄ (downloaded datasets)
```

## üîß Troubleshooting

### Common Issues and Solutions:

#### 1. **API Key Errors**
```python
# Verify your secrets.yaml file
import yaml
with open('secrets.yaml', 'r') as f:
    secrets = yaml.safe_load(f)
    print(f"API Key configured: {'roboflow_api_key' in secrets}")
```

#### 2. **Memory Issues**
```python
# Clear GPU memory
import torch
torch.cuda.empty_cache()

# Reduce batch size
system.update_configuration('model', 'default_batch_size', 8)
```

#### 3. **Dataset Download Issues**
```python
# Check dataset availability
import os
print(f"data.yaml exists: {os.path.exists('datasets/data.yaml')}")
print(f"Dataset directory contents: {os.listdir('datasets/')}")
```

#### 4. **Training Failures**
```python
# Check CUDA availability
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU count: {torch.cuda.device_count()}")
```

#### 5. **RKNN Toolkit2 Issues**
```python
# Check RKNN installation
try:
    import rknn_toolkit2
    print("‚úÖ RKNN Toolkit2 installed successfully")
except ImportError:
    print("‚ùå RKNN Toolkit2 not installed")
    print("Installing from GitHub...")
    !pip install https://github.com/airockchip/rknn-toolkit2/raw/master/rknn-toolkit2/packages/x86_64/rknn_toolkit2-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
```

## üìà Performance Optimization Tips

### For Faster Training:
1. **Use GPU**: Ensure CUDA is available
2. **Optimize Batch Size**: Use `system.get_cuda_status()['optimal_batch_size']`
3. **Reduce Image Size**: Modify `img_size` in config.yaml
4. **Clear Cache**: Run `system.clear_cuda_cache()` between training runs

### For Better Results:
1. **Increase Epochs**: Set higher epoch count for better convergence
2. **Adjust Learning Rate**: Modify in config.yaml
3. **Use Data Augmentation**: Enable in training configuration
4. **Monitor Metrics**: Use TensorBoard for real-time monitoring

## üéØ Quick Start Script (Updated)

For experienced users, here's a quick execution script with updated dependencies:

```python
# Quick execution script with updated dependencies
!git clone https://github.com/trisgelar/pmu-instance-sampah-fuzzy.git
%cd pmu-instance-sampah-fuzzy

# Install dependencies from requirements file
!pip install ultralytics==8.3.174 roboflow==1.2.3 opencv-python==4.11.0.86 matplotlib==3.10.5 seaborn==0.13.2 pandas==2.3.1 numpy==1.26.4 PyYAML==6.0.2 scikit-fuzzy==0.5.0 tensorboard==2.20.0 onnx==1.18.0 onnxruntime==1.22.1 torch==2.4.0+cu124 torchvision==0.19.0+cu124 torchaudio==2.4.0+cu124 pillow==11.3.0 requests==2.32.4 python-dotenv==1.1.1 tqdm==4.67.1 protobuf==4.25.4 flatbuffers==25.2.10 filelock==3.13.1 psutil==7.0.0 scipy==1.16.1 networkx==3.3

# Install RKNN Toolkit2 from GitHub
!pip install https://github.com/airockchip/rknn-toolkit2/raw/master/rknn-toolkit2/packages/x86_64/rknn_toolkit2-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl

# Mount Drive
from google.colab import drive
drive.mount('/content/drive')

# Create secrets (replace with your API key)
import yaml
with open('secrets.yaml', 'w') as f:
    yaml.dump({'roboflow_api_key': 'YOUR_API_KEY'}, f)

# Initialize and run
from main_colab import WasteDetectionSystemColab
system = WasteDetectionSystemColab()

# Prepare datasets
system.dataset_manager.prepare_datasets()

# Train model
run_dir = system.train_and_export_model("v8n", epochs=50, batch_size=16)

# Analyze and visualize
if run_dir:
    system.analyze_training_run(run_dir, "v8n")
    system.run_inference_and_visualization(run_dir, "v8n", num_inference_images=6)
    
    # Try RKNN conversion
    try:
        system.convert_and_zip_rknn_models("v8n")
        print("‚úÖ RKNN conversion completed!")
    except Exception as e:
        print(f"‚ö†Ô∏è  RKNN conversion failed: {e}")

print("‚úÖ Training completed successfully!")
```

## üîó RKNN Toolkit2 Information

### Source:
- **GitHub Repository**: [https://github.com/airockchip/rknn-toolkit2](https://github.com/airockchip/rknn-toolkit2)
- **Wheel File**: `rknn_toolkit2-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl`
- **Direct URL**: [https://github.com/airockchip/rknn-toolkit2/blob/master/rknn-toolkit2/packages/x86_64/rknn_toolkit2-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl](https://github.com/airockchip/rknn-toolkit2/blob/master/rknn-toolkit2/packages/x86_64/rknn_toolkit2-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl)

### Features:
- **Python 3.11 Support**: Compatible with Python 3.11
- **ManyLinux2014**: Optimized for Linux distributions
- **x86_64 Architecture**: For 64-bit systems
- **CUDA Support**: GPU acceleration for model conversion
- **Edge Deployment**: Convert models for Rockchip devices

## üìû Support

If you encounter issues:

1. **Check Logs**: Review the console output for error messages
2. **Verify Configuration**: Ensure all files are properly set up
3. **Check Dependencies**: Verify all packages are installed
4. **Monitor Resources**: Ensure sufficient GPU memory and storage
5. **Review Documentation**: Check the main README.md for additional details
6. **RKNN Issues**: Check the [RKNN Toolkit2 GitHub repository](https://github.com/airockchip/rknn-toolkit2) for updates

## üîó Additional Resources

- **Repository**: [https://github.com/trisgelar/pmu-instance-sampah-fuzzy](https://github.com/trisgelar/pmu-instance-sampah-fuzzy)
- **API Reference**: See `docs/API_REFERENCE.md`
- **Deployment Guide**: See `docs/DEPLOYMENT_GUIDE.md`
- **Troubleshooting**: See `docs/` for additional guides
- **RKNN Toolkit2**: [https://github.com/airockchip/rknn-toolkit2](https://github.com/airockchip/rknn-toolkit2)

---

**Happy Training! üöÄ**

*This guide is designed to help you successfully execute the waste detection system on Google Colab with updated dependencies and RKNN Toolkit2 support. Follow each step carefully and ensure all prerequisites are met before starting.* 
